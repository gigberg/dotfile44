import torch
torch.__version__
exit
import torch
torch.__version__()
torch.__version__
exit
torch
import torchvision
import torch
import torchvision
import torch
import torchvision
n_epochs = 3
batch_size_train = 64
batch_size_test = 1000
learning_rate = 0.01
momentum = 0.5
log_interval = 10
random_seed = 1
torch.backends.cudnn.enabled = False
torch.manual_seed(random_seed)
n_epochs
torchvision.datasets.MNIST('/files/', train=True, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor]))
torch.cuda.is_available()
import torch
x = torch.rand(5, 3)
print(x)
import torch
x = torch.rand(5, 3)
print(x)
ipython
exit
exit()
import torch
torch.cuda.is_available()
import torch
torch.cuda.is_available()
exit()
import torch
torch.cuda.is_available()
exit(0
exit()
import torch
torch.cuda.is_available()
exit()
import torch
exit()
import torch
torch.cuda.is_available()
exit()
import torch
torch.device('cuda' if torch.cuda.is_available() else 'cpu')
import torch
print(torch.__version__)
print(torch.version.cuda) 
import torch
print(torch.__version__)
print(torch.version.cuda)
import torch
print(torch.__version__)
import torch
print(torch.version.cuda)
import torch
print(torch.version.cuda)
torch.device('cuda' if torch.cuda.is_available() else 'cpu')
import torch
torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(torch.version.cuda)
import torch
print(torch.version.cuda)
torch.device('cuda' if torch.cuda.is_available() else 'cpu')
exit()
import torch
torch.device('cuda' if torch.cuda.is_available() else 'cpu')
a = torch.tensor([1.0, 2.222], device='cuda')
a
a = torch.tensor([1.0, 2.222], device='cuda')
a = torch.tensor([1.0, 2.2], device='cuda')
a
exit
torch.cuda.device_count()
exit
import torch
torch.cuda.current_device()
b = torch.tensor([1., 2.]).cuda()
b
 b2 = torch.tensor([1., 2.]).to(device='cuda')
cuda = torch.device('cuda') 
b = torch.tensor([1., 2.]).cuda()
b
exit
abs()
import pandas as pd
from attrdict import AttrDict
import pandas as pd
from attrdict import AttrDict
get_linear_schedule_with_warmup
compute_metrics
logger = logging.getLogger(__name__)
import spam
import numpy
pydoc numpy
q
import nlpcode/pycharm_project_544/test.py
import nlpcode.pycharm_project_544.test
_private_1('dd')
import sys
sys.path
exit
dir()
__build__
 cd /home/zhoujiaming/kyoto/nlpcode/pycharm_project_544/hh ; /usr/bin/env /home/zhoujiaming/anaconda3/envs/mann/bin/python /home/zhoujiaming/.vscode-server/extensions/ms-python.python-2022.2.1924087327/pythonFiles/lib/python/debugpy/launcher 46811 -- /home/zhoujiaming/kyoto/nlpcode/pycharm_project_544/hh/hhtest.py 
import hh
import requess
import requests
touch app.py
import torch
torch.version.cuda
exit()
import torch
torch.version.cuda
import torch
torch.cuda.aviable
torch.cuda.is_available()
torch.zeros(1).cuda()
exit
exit()
import lelveldb
import leveldb
exit()
import leveldb
exit()
import matplotlib.pyplot as plt
plt.plot([1, 2, 3, 4])
# plt.ylabel('some numbers')
# plt.show()
plt.show()
exit()
import re
re.findall(r'\W*', '...words...')
re.findall(r'\W+', '..words...')
re.findall(r'\w+', '..words...')
re.findall(r'\W+', '..words...')
re.findall(r'\W?', '..words...')
import torch
print(torch.FloatTensor(2,3).type()) #构建一个2*3 Float类型的张量
print(torch.DoubleTensor(2,3).type()) #构建一个2*3 Double类型的张量
print(torch.HalfTensor (2,3).type()) #构建一个2*3 HalfTenso类型的张量
print(torch.ByteTensor(2,3).type()) #构建一个2*3 Byte类型的张量
print(torch.CharTensor(2,3).type()) #构建一个2*3 Char类型的张量
print(torch.ShortTensor(2,3).type()) #构建一个2*3 Short类型的张量
print(torch.IntTensor(2,3).type()) #构建一个2*3 Int类型的张量
print(torch.LongTensor(2,3).type()) #构建一个2*3 Long类型的张量
exit()
import sys
 
import sys
 
sys.stdout.write("stdout1")
sys.stderr.write("stderr1")
sys.stdout.write("stdout2")
sys.stderr.write("stderr2")
exit
exit()
import sys
sys.stdout("x")
sys.stdout.write("x")
exit()
exit
exit()
import torch
exit
exit()
'HOME' in os.environ
import os
'HOME' in os.environ
'PYTHONHOME' in os.environ
'PYTHONPATH' in os.environ
exit
exit()
import paddle
import paddlepaddle
import paddlepaddle-gpu
import paddle
import paddlenlp
from paddlenlp.utils.env import MODEL_HOME
MODEL_HOME
from paddlenlp.transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
# 单句转换（单条数据）
print(tokenizer(text='天气不错')) # {'input_ids': [101, 1921, 3698, 679, 7231, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0]}
# 句对转换（单条数据）
print(tokenizer(text='天气',text_pair='不错')) # {'input_ids': [101, 1921, 3698, 102, 679, 7231, 102], 'token_type_ids': [0, 0, 0, 0, 1, 1, 1]}
# 单句转换（多条数据）
print(tokenizer(text=['天气','不错'])) # [{'input_ids': [101, 1921, 3698, 102], 'token_type_ids': [0, 0, 0, 0]},
                                      #  {'input_ids': [101, 679, 7231, 102], 'token_type_ids': [0, 0, 0, 0]}]
from paddlenlp.transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-wwm-ext-chinese')
# 单句转换（单条数据）
print(tokenizer(text='天气不错')) # {'input_ids': [101, 1921, 3698, 679, 7231, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0]}
# 句对转换（单条数据）
print(tokenizer(text='天气',text_pair='不错')) # {'input_ids': [101, 1921, 3698, 102, 679, 7231, 102], 'token_type_ids': [0, 0, 0, 0, 1, 1, 1]}
# 单句转换（多条数据）
print(tokenizer(text=['天气','不错'])) # [{'input_ids': [101, 1921, 3698, 102], 'token_type_ids': [0, 0, 0, 0]},
                                      #  {'input_ids': [101, 679, 7231, 102], 'token_type_ids': [0, 0, 0, 0]}]
from paddlenlp.transformers import BertTokenizer
from paddlenlp.datasets import load_dataset
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
train_ds = load_dataset('lcqmc', splits='train')
print(train_ds[0]) # {'query': '喜欢打篮球的男生喜欢什么样的女生', 'title': '爱打篮球的男生喜欢什么样的女生', 'label': 1}
def convert_example(example, tokenizer):
    tokenized_example = tokenizer(
                            text=example['query'],
                            text_pair=example['title'])
    # 加上label用于训练
    tokenized_example['label'] = [example['label']]
    return tokenized_example
from functools import partial
trans_func = partial(
    convert_example,
    tokenizer=tokenizer)
train_ds.map(trans_func)
print(train_ds[0]) # {'input_ids': [101, 1599, 3614, 2802, 5074, 4413, 4638, 4511, 4495,
                   #                1599, 3614, 784, 720, 3416, 4638, 1957, 4495, 102,
                   #                4263, 2802, 5074, 4413, 4638, 4511, 4495, 1599, 3614,
                   #                784, 720, 3416, 4638, 1957, 4495, 102],
                   #  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   #                     0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                   #  'label': [1]}
def convert_examples(examples, tokenizer):
    querys = [example['query'] for example in examples]
    titles = [example['title'] for example in examples]
    tokenized_examples = tokenizer(text=querys, text_pair=titles,return_dict=False)
    # 加上label用于训练
    for idx in range(len(tokenized_examples)):
        tokenized_examples[idx]['label'] = [examples[idx]['label']]
    return tokenized_examples
from functools import partial
trans_func = partial(convert_examples, tokenizer=tokenizer)
train_ds.map(trans_func, batched=True)
exit()
import readline
exit()
 import keras as k
import keras as k
import pytorch
import spacy
print(spacy.__version__)
import spacy
print(spacy.__version__)
import neuralcoref
bt
q
import neuralcoref
import spacy
print(spacy.__version__)
import spacy
print(spacy.__version__)
import neuralcoref
import amrlib
stog = amrlib.load_stog_model()
q
import amrlib
stog = amrlib.load_stog_model()
import amrlib
stog = amrlib.load_stog_model()
graphs = stog.parse_sents(['This is a test of the system.', 'This is a second sentence.'])
for graph in graphs:
    print(graph)
spacy.load('../en_core_web_sm-2.3.0/en_core_web_sm/en_core_web_sm-2.3.0')
import spacy
print(spacy.__version__)
spacy.load('../en_core_web_sm-2.3.0/en_core_web_sm/en_core_web_sm-2.3.0')
import nltk
import os
os.environ['http_proxy'] = "http://127.0.0.1:7890"
os.environ['https_proxy'] = "http://127.0.0.1:7890"
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
nltk.download('omw-1.4')
import os
os.environ['https_proxy'] = "http://127.0.0.1:7890"
import nltk
nltk.download('wordnet')
import neuralcoref
importneuralcoref
import neuralcoref
import spacy
import neuralcoref
nlp = spacy.load('en_core_web_sm')
neuralcoref.add_to_pipe(nlp)
import neuralcoref
import spacy
import en_core_web_sm
nlp=spacy.load('en')
import neuralcoref
import spacy
import neuralcoref
import spacy
import spacynlp = spacy.load('en_core_web_sm')
nlp = spacy.load('en_core_web_sm')
neuralcoref.add_to_pipe(nlp)
doc = nlp('i need to find out the date and time for my swimming_activity. i have two which one i have one for the_14th at 6pm and one for the_12th at 7pm')
import neuralcoref
nltk.download('wordnet')
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
import neuralcoref
import spacy
import json
nlp = spacy.load('../en_core_web_sm-2.3.0/en_core_web_sm/en_core_web_sm-2.3.0')
# Add neural coref to SpaCy's pipe
import neuralcoref
neuralcoref.add_to_pipe(nlp)
doc = nlp('i need to find out the date and time for my swimming_activity. i have two which one i have one for the_14th at 6pm and one for the_12th at 7pm')
ll
import gpt4all
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", use_fast=True)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", use_fast=True)
str = "This is a tokenization example"
encoded = tokenizer(str)
encoded.words
encoded.words()
encoded.offsets
encoded.words()
encoded.ids
tokenizer('hey how's it going')
tokenizer("hey how's it going")
tokenizer("hey how's it going").words()
tokenizer("hey how's it going").ids
tokenizer("hey how's it going").words
tokenizer("hey how's it going").words()
tokenizer("hey how's it going").decode()
tokenizer("hey how's it going").words
tokenizer("hey how's it going").words()
tokenize.decode([101, 4931, 2129, 1005, 1055, 2009, 2183, 102])
tokenize.decodre([101, 4931, 2129, 1005, 1055, 2009, 2183, 102])
tokenizer.decodre([101, 4931, 2129, 1005, 1055, 2009, 2183, 102])
tokenizer.decore([101, 4931, 2129, 1005, 1055, 2009, 2183, 102])
tokenizer.decode([101, 4931, 2129, 1005, 1055, 2009, 2183, 102])
tokenizer("hey how's it going").words()
tokenizer("hey how's it going").word_ids()
tokenizer.decode([101, 4931, 2129, 1005, 1055, 2009, 2183, 102])
tokenizer("hey how's it going")
tokenizer("hey how's it going").words
if 0: print(2)
if 0:;
if 0:
    print(2)
import os
import torch
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
print(f"Using GPU is CUDA:{os.environ['CUDA_VISIBLE_DEVICES']}")
for i in range(torch.cuda.device_count()):
    info = torch.cuda.get_device_properties(i)
    print(f"CUDA:{i} {info.name}, {info.total_memory / 1024 ** 2}MB")
device = torch.device("cuda:0")
x = {}
y = {}
y[1] = 2
y
x.update(y)
x
y[1]=3
y
x
import torch
# some inputs
x = torch.rand(10, 3)
# some targets
y = torch.rand(10, 1)
# some parameter
W = torch.nn.Parameter(torch.rand(3, 1))
# total loss
total_loss = torch.tensor([0.0])
# some condition
if torch.rand(1) > 0.5:
    loss = torch.sum((torch.matmul(x, W) - y) ** 2)
    total_loss += loss
total_loss.backward()
total_loss
import torch
# some inputs
x = torch.rand(10, 3)
# some targets
y = torch.rand(10, 1)
# some parameter
W = torch.nn.Parameter(torch.rand(3, 1))
# total loss
total_loss = torch.tensor([0.0])
# some condition
if torch.rand(1) > 0.5:
    loss = torch.sum((torch.matmul(x, W) - y) ** 2)
    total_loss += loss
total_loss
total_loss.backward()
from transformers import BertTokenizer
from transformers import BertModel
from transformers import BertTokenizer
from transformers import BertModel
import os
os.getcwd()
q
from transformers import BertModel
from transformers import BertTokenizer
BertTokenizer.from_pretrained('bert-base-uncased').save_pretrained('bert-base-uncased')
BertModel.from_pretrained('bert-base-uncased').save_pretrained('bert-base-uncased')
from transformers import BertModel
BertModel.from_pretrained('bert-base-uncased').save_pretrained('./pretrained/bert-base-uncased')
import os
os.path.isdir('~/.pretrain/bert-base-uncased')
os.path.isdir('~/.pretrained/bert-base-uncased')
os.path.isdir('/home/zhoujiaming/.pretrained/bert-base-uncased')
os.path.isdir('~/.pretrained/bert-base-uncased')
{1:2}+{3:4}
import torch
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
x = torch.arange(-5, 5, 0.1).view(-1, 1)
y = -5 * x + 0.1 * torch.randn(x.size())
model = torch.nn.Linear(1, 1)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)
def train_model(iter):
    for epoch in range(iter):
        y1 = model(x)
        loss = criterion(y1, y)
        writer.add_scalar("Loss/train", loss, epoch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
train_model(10)
writer.flush()
x = torch.arange(-5, 5, 0.1).view(-1, 1)
y = -5 * x + 0.1 * torch.randn(x.size())
model = torch.nn.Linear(1, 1)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)
def train_model(iter):
    for epoch in range(iter):
        y1 = model(x)
        loss = criterion(y1, y)
        writer.add_scalar("Loss/train", loss, epoch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
train_model(10)
writer.flush()
cat /path/to/cudnn/include/cudnn.h | grep CUDNN_MAJOR -A 2
import torch
print(torch.backends.cudnn.version())
import torch
# Define input tensor (batch_size, channels, height, width)
input_tensor = torch.randn(1, 1, 3, 3)
# Create a lower triangle mask
mask = torch.tril(torch.ones_like(input_tensor))
# Apply the mask to the input
masked_input = input_tensor * mask
# Reshape the masked input for RNN processing
reshaped_input = masked_input.view(1, 1, -1, 3)  # Reshape to (batch_size, channels, height*width, width)
# Define an RNN model to process the masked input
rnn = nn.RNN(input_size=3, hidden_size=3, num_layers=1, batch_first=True)
# Process the masked input using RNN
output, hidden = rnn(reshaped_input)
print(output.shape)  # Shape of the output from RNN
import spacy
spacy_en = spacy.load("en_core_web_sm")
random
torch.random
# Example of target with class indices
loss = nn.CrossEntropyLoss()
input = torch.randn(3, 5, requires_grad=True)
target = torch.empty(3, dtype=torch.long).random_(5)
output = loss(input, target)
output.backward()
# Example of target with class probabilities
input = torch.randn(3, 5, requires_grad=True)
target = torch.randn(3, 5).softmax(dim=1)
output = loss(input, target)
type(__builtdins__)
type(__builtins__)
 cd /home/zhoujiaming/kyoto/nlpcode/SemTransModel/SemTM ; /usr/bin/env /home/zhoujiaming/anaconda3/envs/hitrans/bin/python /usr/local/zhoujiaming/.vscode-server/extensions/ms-python.debugpy-2024.2.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 33969 -- /home/zhoujiaming/kyoto/nlpcode/SemTransModel/SemTM/main.py 
__builtin__
__builtins__
__builtin__
__builtins__
import __builtin__
__debug__
# Load your usual SpaCy model (one of SpaCy English models)
import spacy
nlp = spacy.load('en')
# load NeuralCoref and add it to the pipe of SpaCy's model
import neuralcoref
coref = neuralcoref.NeuralCoref(nlp.vocab)
nlp.add_pipe(coref, name='neuralcoref')
import spacy
nlp = spacy.load('en_core_web_sm')
import neuralcoref
coref = neuralcoref.NeuralCoref(nlp.vocab)
nlp.add_pipe(coref, name='neuralcoref')
